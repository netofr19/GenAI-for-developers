{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JI4uPsPbmiA6"
   },
   "source": [
    "# CHAPTER 07 - Advanced Text Generation Techniques and Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huyxdo-z6jXc"
   },
   "source": [
    "## Model I/O: Loading Quantized Models with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1737985707584,
     "user": {
      "displayName": "Francisco Procópio",
      "userId": "12618119772482679330"
     },
     "user_tz": 180
    },
    "id": "OxcwCaGaBNUy",
    "outputId": "02fc9337-9cef-475a-9e90-036c77fe8981"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1043,
     "status": "ok",
     "timestamp": 1737985717333,
     "user": {
      "displayName": "Francisco Procópio",
      "userId": "12618119772482679330"
     },
     "user_tz": 180
    },
    "id": "dqToVEV7_mVv"
   },
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1527,
     "status": "ok",
     "timestamp": 1737985936031,
     "user": {
      "displayName": "Francisco Procópio",
      "userId": "12618119772482679330"
     },
     "user_tz": 180
    },
    "id": "4Nz1HrsLBW6J",
    "outputId": "91e77dba-26de-4dc0-fcbb-09223931668d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi Maarten! 1 + 1 equals 2.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 23, 'total_tokens': 37, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-007f8f4f-c303-4b80-abb7-41b4dd79fdc7-0', usage_metadata={'input_tokens': 23, 'output_tokens': 14, 'total_tokens': 37, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\n",
    "    \"Hi! My name is Maarten. What is 1 + 1?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13C23rsPDjdO"
   },
   "source": [
    "### A Single Link in the Chain: Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1737986169524,
     "user": {
      "displayName": "Francisco Procópio",
      "userId": "12618119772482679330"
     },
     "user_tz": 180
    },
    "id": "jePWeIDSCMMc"
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1737987420290,
     "user": {
      "displayName": "Francisco Procópio",
      "userId": "12618119772482679330"
     },
     "user_tz": 180
    },
    "id": "XxvbiLgYDFgo"
   },
   "outputs": [],
   "source": [
    "# Create a prompt template with the \"input_prompt\" variable\n",
    "template = \"\"\"\n",
    "Question: {input_prompt}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input_prompt\"],\n",
    "    template=template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 294,
     "status": "ok",
     "timestamp": 1737987427766,
     "user": {
      "displayName": "Francisco Procópio",
      "userId": "12618119772482679330"
     },
     "user_tz": 180
    },
    "id": "yRZaoLzGHXxu"
   },
   "outputs": [],
   "source": [
    "basic_chain = prompt_template | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 1083,
     "status": "ok",
     "timestamp": 1737987542098,
     "user": {
      "displayName": "Francisco Procópio",
      "userId": "12618119772482679330"
     },
     "user_tz": 180
    },
    "id": "6dPTsxuJHtRn",
    "outputId": "bc047fd6-647c-4d8a-b527-361e0d08f425"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Maarten! The answer to 1 + 1 is 2.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = basic_chain.invoke(\n",
    "    {\n",
    "        \"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1737987859084,
     "user": {
      "displayName": "Francisco Procópio",
      "userId": "12618119772482679330"
     },
     "user_tz": 180
    },
    "id": "DJEkHTGSIQTF"
   },
   "outputs": [],
   "source": [
    "# Create a Chain that creates our business' name\n",
    "template = \"\"\"\n",
    "Create a funny name for a business that sells {product}\n",
    "\"\"\"\n",
    "\n",
    "name_prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"product\"]\n",
    ")\n",
    "\n",
    "product_chain = name_prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 607,
     "status": "ok",
     "timestamp": 1737987992146,
     "user": {
      "displayName": "Francisco Procópio",
      "userId": "12618119772482679330"
     },
     "user_tz": 180
    },
    "id": "qlcOvzDiJh_E",
    "outputId": "46ec0fa3-1da3-4224-fca0-142f83b7dbc6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"The Wheely Silly Car Co.\"'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = product_chain.invoke(\n",
    "    {\"product\": \"cars\"}\n",
    ")\n",
    "\n",
    "output.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCUpnxoBKE5O"
   },
   "source": [
    "### A Chain with Multiple Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1737989418516,
     "user": {
      "displayName": "Francisco Procópio",
      "userId": "12618119772482679330"
     },
     "user_tz": 180
    },
    "id": "v3CUD4V_Ji6K"
   },
   "outputs": [],
   "source": [
    "from langchain import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1737989367258,
     "user": {
      "displayName": "Francisco Procópio",
      "userId": "12618119772482679330"
     },
     "user_tz": 180
    },
    "id": "m_4vlOUQLCAV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\francisco.procopio\\AppData\\Local\\Temp\\ipykernel_5684\\3282791426.py:12: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  title_chain = LLMChain(llm=chat_model, prompt=title_prompt, output_key=\"title\")\n"
     ]
    }
   ],
   "source": [
    "# Create a chain for the title of our story\n",
    "template_title = \"\"\"\n",
    "Create a title for a story about {summary}. Only return the title.\n",
    "Title:\n",
    "\"\"\"\n",
    "\n",
    "title_prompt = PromptTemplate(\n",
    "    template=template_title,\n",
    "    input_variables=[\"summary\"]\n",
    ")\n",
    "\n",
    "title_chain = LLMChain(llm=chat_model, prompt=title_prompt, output_key=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 619,
     "status": "ok",
     "timestamp": 1737989500960,
     "user": {
      "displayName": "Francisco Procópio",
      "userId": "12618119772482679330"
     },
     "user_tz": 180
    },
    "id": "-GtP12N9LkS2",
    "outputId": "3d29dcd1-3168-433d-a879-63d1dc22a836"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': 'a girl that lost her mother', 'title': '\"Eclipse of the Heart\"'}\n"
     ]
    }
   ],
   "source": [
    "title = title_chain.invoke(\n",
    "    {\n",
    "        \"summary\": \"a girl that lost her mother\"\n",
    "    }\n",
    ")\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 353,
     "status": "ok",
     "timestamp": 1737989478274,
     "user": {
      "displayName": "Francisco Procópio",
      "userId": "12618119772482679330"
     },
     "user_tz": 180
    },
    "id": "p0ujNJp5MVGn"
   },
   "outputs": [],
   "source": [
    "# Create a chain for the character description using the summary and title\n",
    "template_character = \"\"\"\n",
    "Describe the main character of a story about {summary} with the title {title}.\n",
    "Use only two sentences.\n",
    "\"\"\"\n",
    "\n",
    "character_prompt = PromptTemplate(\n",
    "    template=template_character,\n",
    "    input_variables=[\"summary\", \"title\"]\n",
    ")\n",
    "\n",
    "character_chain = LLMChain(\n",
    "    llm = chat_model,\n",
    "    prompt = character_prompt,\n",
    "    output_key=\"character\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1403,
     "status": "ok",
     "timestamp": 1737989544764,
     "user": {
      "displayName": "Francisco Procópio",
      "userId": "12618119772482679330"
     },
     "user_tz": 180
    },
    "id": "uhlaB-hGNKDe",
    "outputId": "bb58d632-640b-4330-f5ed-cfaf20541d9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'a girl that lost her mother',\n",
       " 'title': '\"Eclipse of the Heart\"',\n",
       " 'character': 'The main character, Eliza, is a young girl who is struggling to cope with the loss of her mother and feels lost and heartbroken without her. She must navigate through her grief and find a way to heal as she searches for the light in the darkness of her life.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character = character_chain.invoke(\n",
    "    {\n",
    "        \"summary\": \"a girl that lost her mother\",\n",
    "        \"title\": title['title']\n",
    "    }\n",
    ")\n",
    "\n",
    "character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1737989571823,
     "user": {
      "displayName": "Francisco Procópio",
      "userId": "12618119772482679330"
     },
     "user_tz": 180
    },
    "id": "J-LmLl5kNNZz"
   },
   "outputs": [],
   "source": [
    "# Create a chain for the story using the summary, title, and character description\n",
    "template_story = \"\"\"\n",
    "Create a story about {summary} with the title {title}. The main character is: {character}.\n",
    "Only return the story and it cannot be longer than one paragraph.\n",
    "\"\"\"\n",
    "\n",
    "story_prompt = PromptTemplate(\n",
    "    template=template_story,\n",
    "    input_variables=[\"summary\", \"title\", \"character\"]\n",
    ")\n",
    "\n",
    "story_chain = LLMChain(\n",
    "    llm = chat_model,\n",
    "    prompt = story_prompt,\n",
    "    output_key=\"story\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1850,
     "status": "ok",
     "timestamp": 1737990054500,
     "user": {
      "displayName": "Francisco Procópio",
      "userId": "12618119772482679330"
     },
     "user_tz": 180
    },
    "id": "aom2WYozOGj8",
    "outputId": "bad84531-5081-4147-9de4-ecbd6fb444e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'a girl that lost her mother',\n",
       " 'title': '\"Eclipse of the Heart\"',\n",
       " 'character': 'The main character, Eliza, is a young girl who is struggling to cope with the loss of her mother and feels lost and heartbroken without her. She must navigate through her grief and find a way to heal as she searches for the light in the darkness of her life.',\n",
       " 'story': 'Eliza was just a young girl when she experienced the eclipse of her heart - the moment her mother passed away. From that day on, she felt like the sun had disappeared from her life, leaving her in a never-ending darkness. Eliza struggled to find her way through the confusing maze of grief and heartache, but deep down, she knew that she had to keep searching for the light that would slowly begin to shine through the shadows. As she navigated the storm of emotions within her, Eliza learned to lean on the love and support of those around her, slowly allowing her heart to heal and find its way back to the brightness that once filled her world.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story = story_chain.invoke(\n",
    "    {\n",
    "        \"summary\": \"a girl that lost her mother\",\n",
    "        \"title\": title['title'],\n",
    "        \"character\": character['character']\n",
    "    }\n",
    ")\n",
    "\n",
    "story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 369,
     "status": "ok",
     "timestamp": 1737989596944,
     "user": {
      "displayName": "Francisco Procópio",
      "userId": "12618119772482679330"
     },
     "user_tz": 180
    },
    "id": "pIX9f0TTOYGJ"
   },
   "outputs": [],
   "source": [
    "complete_chain = title_chain | character_chain | story_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3168,
     "status": "ok",
     "timestamp": 1737989610824,
     "user": {
      "displayName": "Francisco Procópio",
      "userId": "12618119772482679330"
     },
     "user_tz": 180
    },
    "id": "Slf-9BukOtqK",
    "outputId": "fb7bbaff-90e7-44f0-a84f-475c4c93bac6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'the best football team in the world',\n",
       " 'title': '\"Champions of the Pitch\"',\n",
       " 'character': 'The main character of \"Champions of the Pitch\" is a skilled and determined striker who leads the team to victory with his exceptional talent and leadership on the field. They are known for their intense drive and passion for the game, setting the standard for excellence in the world of football.',\n",
       " 'story': 'In the world of football, there is one team that reigns supreme - the Champions of the Pitch. Led by their skilled and determined striker, they dominate the field with exceptional talent and unrivaled leadership. With an intense drive and passion for the game, they set the standard for excellence in the world of football. Their journey to victory is marked by unforgettable moments of triumph and their legacy as the best football team in the world is etched in history forever.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_chain.invoke(\n",
    "     \"the best football team in the world\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhQaJB3TRqAQ"
   },
   "source": [
    "## Memory: Helping LLMs to Remember Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "YFrFRb3dOy3q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Maarten! 1 + 1 equals 2.\n"
     ]
    }
   ],
   "source": [
    "# Let's give the LLM our name\n",
    "output = basic_chain.invoke({\n",
    "    \"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"\n",
    "})\n",
    "\n",
    "resp = output.content\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I do not know your name. Can you please tell me?\n"
     ]
    }
   ],
   "source": [
    "# Next, we ask the LLM to reproduce the name\n",
    "output = basic_chain.invoke(\n",
    "    {\n",
    "        \"input_prompt\": \"What is my name?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "resp = output.content\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an updated prompt template to include a chat history\n",
    "template = \"\"\"\n",
    "Current conversation:\\n\\n{chat_history}\\n\\n{input_prompt}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template = template,\n",
    "    input_variables = [\"input_prompt\", \"chat_history\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the type of memory we will use\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "# Chain the LLM, prompt, and memory together\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=basic_chain,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'Hi! My name is Maarten. What is 1 + 1?',\n",
       " 'chat_history': '',\n",
       " 'text': 'Hi Maarten! The answer to 1 + 1 is 2. How can I assist you further?'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\n",
    "    \"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'what is my name?',\n",
       " 'chat_history': 'Human: Hi! My name is Maarten. What is 1 + 1?\\nAI: Hi Maarten! The answer to 1 + 1 is 2. How can I assist you further?',\n",
       " 'text': 'Your name is Maarten.'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does the LLM remember the name we gave it?\n",
    "llm_chain.invoke({\n",
    "    \"input_prompt\": \"what is my name?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowed Conversation Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\francisco.procopio\\AppData\\Local\\Temp\\ipykernel_5684\\3826035775.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(\n"
     ]
    }
   ],
   "source": [
    "# Retain only the last 2 conversations in memory\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=2,\n",
    "    memory_key=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain the LLM, prompt, and memory together\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=basic_chain,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'Hi! My name is Maarten and I am 33 years old.',\n",
       " 'chat_history': '',\n",
       " 'text': \"Hi Maarten! Nice to meet you. What's on your mind today?\"}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask two questions and generate two conversations in its memory\n",
    "llm_chain.invoke({\n",
    "    \"input_prompt\": \"Hi! My name is Maarten and I am 33 years old.\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'what is 3 + 3?',\n",
       " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old.\\nAI: Hi Maarten! Nice to meet you. What's on your mind today?\",\n",
       " 'text': '3 + 3 equals 6.'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\n",
    "    \"input_prompt\": \"what is 3 + 3?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'what is my name?',\n",
       " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old.\\nAI: Hi Maarten! Nice to meet you. What's on your mind today?\\nHuman: what is 3 + 3?\\nAI: 3 + 3 equals 6.\",\n",
       " 'text': 'Your name is Maarten.'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether it knows the name we gave it\n",
    "llm_chain.invoke({\n",
    "    \"input_prompt\": \"what is my name?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'what is my age?',\n",
       " 'chat_history': 'Human: what is 3 + 3?\\nAI: 3 + 3 equals 6.\\nHuman: what is my name?\\nAI: Your name is Maarten.',\n",
       " 'text': 'Sorry, I cannot answer that question as I do not have that information.'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether it knows the age we gave it\n",
    "llm_chain.invoke({\n",
    "    \"input_prompt\": \"what is my age?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize the conversations and update with the new lines.\n",
      "\n",
      "Current summary:\n",
      "{summary}\n",
      "\n",
      "new lines of conversation:\n",
      "{new_lines}\n",
      "\n",
      "New summary:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a summary prompt template\n",
    "summary_prompt_template = \"\"\"\n",
    "Summarize the conversations and update with the new lines.\n",
    "\n",
    "Current summary:\n",
    "{summary}\n",
    "\n",
    "new lines of conversation:\n",
    "{new_lines}\n",
    "\n",
    "New summary:\n",
    "\"\"\"\n",
    "\n",
    "summary_prompt = PromptTemplate(\n",
    "    template=summary_prompt_template,\n",
    "    input_variables=[\"new_lines\", \"summary\"]\n",
    ")\n",
    "\n",
    "print(summary_prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the type of memory we will use\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=chat_model,\n",
    "    memory_key=\"chat_history\",\n",
    "    prompt=summary_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain the LLM, prompt, and memory together\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=chat_model,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'Hi! My name is Maarten. What is 1 + 1?',\n",
       " 'chat_history': '',\n",
       " 'text': 'Hi Maarten! 1 + 1 equals 2. Do you have any other questions?'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a conversation and ask for the name\n",
    "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is my name?',\n",
       " 'chat_history': 'Maarten asks what 1 + 1 is, and the AI responds that it equals 2. The AI then asks if Maarten has any other questions.',\n",
       " 'text': \"I'm sorry, I am not equipped to know your name. How can I assist you with something else?\"}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What was the first question I asked?',\n",
       " 'chat_history': 'Maarten asks what 1 + 1 is, and the AI responds that it equals 2. The AI then asks if Maarten has any other questions. Maarten then asks for his name, and the AI apologizes for not knowing it. The AI offers its assistance with anything else Maarten needs help with.',\n",
       " 'text': 'The first question you asked was \"What is 1 + 1?\"'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether it has summarized everything thus far\n",
    "llm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': \"Maarten inquires about the first question he asked, and the AI confirms that it was about the sum of 1 + 1. The AI is attentive to Maarten's queries and offers help with any other questions or needs he may have.\"}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what the summary is thus far\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents: Creating a System of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ReAct template\n",
    "react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times.)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=react_template,\n",
    "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, Tool\n",
    "from langchain.tools import DuckDuckGoSearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can create the tool to pass to an agent\n",
    "search = DuckDuckGoSearchResults()\n",
    "\n",
    "search_tool = Tool(\n",
    "    name=\"duckduck\",\n",
    "    description=\"A web search engine. Use this to as a search engine for general queries.\",\n",
    "    func=search.run\n",
    ")\n",
    "\n",
    "# Prepare tools\n",
    "tools = load_tools(['llm-math'], llm=openai_llm)\n",
    "tools.append(search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='Calculator', description='Useful for when you need to answer questions about math.', func=<bound method Chain.run of LLMMathChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000018790831D60>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000018790833A10>, root_client=<openai.OpenAI object at 0x0000018790717E90>, root_async_client=<openai.AsyncOpenAI object at 0x0000018790831DC0>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}))>, coroutine=<bound method Chain.arun of LLMMathChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000018790831D60>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000018790833A10>, root_client=<openai.OpenAI object at 0x0000018790717E90>, root_async_client=<openai.AsyncOpenAI object at 0x0000018790831DC0>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}))>),\n",
       " Tool(name='duckduck', description='A web search engine. Use this to as a search engine for general queries.', func=<bound method BaseTool.run of DuckDuckGoSearchResults(api_wrapper=DuckDuckGoSearchAPIWrapper(region='wt-wt', safesearch='moderate', time='y', max_results=5, backend='auto', source='text'))>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the ReAct agent\n",
    "agent = create_react_agent(\n",
    "    openai_llm,\n",
    "    tools,\n",
    "    prompt\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I should use a web search engine to find the current price of a MacBook Pro in USD and then use a calculator to convert it to EUR.\n",
      "Action: duckduck\n",
      "Action Input: \"current price of MacBook Pro in USD\"\u001b[0m\u001b[33;1m\u001b[1;3msnippet: With power, performance, and premium style, the MacBook Pro represents the pinnacle of Apple's laptop offerings. This comprehensive guide breaks down current MacBook Pro pricing across available sizes, processors, storage capacities, and more. You'll learn how choices at checkout impact final cost, find savings opportunities, and gain insight into Apple's pricing philosophy on its ..., title: How Much Does a MacBook Pro Cost? - The Pricer, link: https://www.thepricer.org/how-much-does-a-macbook-pro-cost/, snippet: Apple's 2024 MacBook Pro 16-inch can be equipped with an M4 Pro or M4 Max chip. Retail prices start at $2,499 USD, but every configuration is on sale, with the best prices in this guide knocking hundred of dollars off the laptop of your choosing. You can also browse a selection of the top 16-inch MacBook Pro deals in our dedicated roundup., title: MacBook Pro 16-inch 2024 M4 Best Sale Price Deals, link: https://prices.appleinsider.com/macbook-pro-16-inch-m4, snippet: As of April 2024, the latest MacBook Pro 14-inch M3 model started with a retail price of 1,599 U.S. dollars while the 16-inch M3 Pro version began with a retail price of 2,499 U.S. dollars. This ..., title: Apple MacBook Pro price comparison 2024 | Statista, link: https://www.statista.com/statistics/1360426/apple-macbook-pro-price-comparison/, snippet: The M4 Max MacBook Pro is Apple's most powerful option. At the moment, only the space black variant is available at this price, though to can grab the silver variant on sale for slightly more as well., title: Best MacBook Deals: Grab the Latest M3 and M4 Models for Less, link: https://www.cnet.com/deals/best-macbook-deals/\u001b[0m\u001b[32;1m\u001b[1;3mAction: Calculator\n",
      "Action Input: $2,499 USD * 0.85 EUR/USD\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 2124.15\u001b[0m\u001b[32;1m\u001b[1;3mFinal Answer: The current price of a MacBook Pro in USD is $2,499. It would cost approximately 2124.15 EUR with an exchange rate of 0.85 EUR for 1 USD.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD.',\n",
       " 'output': 'The current price of a MacBook Pro in USD is $2,499. It would cost approximately 2124.15 EUR with an exchange rate of 0.85 EUR for 1 USD.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the price of a MacBook Pro?\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD.\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMv7P7IeK28fnYjvlCoCFK3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
