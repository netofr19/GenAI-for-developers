{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RouterChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fis_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Você é um professor de física muito inteligente. Você é ótimo em responder perguntas sobre física de forma concisa e fácil de entender.\n",
    "    Quando você não sabe a resposta para uma pergunta, você admite que não sabe.\n",
    "\n",
    "    Aqui está uma pergunta: {input}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "mat_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Você é um matemático muito bom. Você é ótimo em responder perguntas de matemática. Você é tão bom porque consegue decompor problemas difíceis em suas partes componentes, responder às partes componentes e depois juntá-las para responder à pergunta mais ampla.\n",
    "\n",
    "    Aqui está uma pergunta: {input}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "hist_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Você é um historiador muito bom. Você tem um excelente conhecimento e compreensão de pessoas, eventos e contextos de uma variedade de períodos históricos. Você tem a capacidade de pensar, refletir, debater, discutir e avaliar o passado. Você tem respeito pela evidência histórica e a capacidade de usá-la para apoiar suas explicações e julgamentos.\n",
    "\n",
    "    \n",
    "    Aqui está uma pergunta: {input}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        'name': 'Fisica',\n",
    "        'description': 'ideal para responder perguntas sobre física',\n",
    "        'prompt_template': fis_template\n",
    "    },\n",
    "    {\n",
    "        'name': 'Matemática',\n",
    "        'description': 'ideal para responder perguntas sobre matemática',\n",
    "        'prompt_template': mat_template\n",
    "    },\n",
    "    {\n",
    "        'name': 'História',\n",
    "        'description': 'ideal para responder perguntas sobre história',\n",
    "        'prompt_template': hist_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\francisco.procopio\\AppData\\Local\\Temp\\ipykernel_8020\\936010572.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=chat, prompt=info['prompt_template'], verbose=True)\n"
     ]
    }
   ],
   "source": [
    "chains_destino = {}\n",
    "for info in prompt_infos:\n",
    "    chain = LLMChain(llm=chat, prompt=info['prompt_template'], verbose=True)\n",
    "    chains_destino[info['name']] = chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fisica': LLMChain(verbose=True, prompt=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='\\n    Você é um professor de física muito inteligente. Você é ótimo em responder perguntas sobre física de forma concisa e fácil de entender.\\n    Quando você não sabe a resposta para uma pergunta, você admite que não sabe.\\n\\n    Aqui está uma pergunta: {input}\\n    '), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001961EC5E6F0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001961E90A900>, root_client=<openai.OpenAI object at 0x000001961E453440>, root_async_client=<openai.AsyncOpenAI object at 0x000001961EC5E750>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}),\n",
       " 'Matemática': LLMChain(verbose=True, prompt=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='\\n    Você é um matemático muito bom. Você é ótimo em responder perguntas de matemática. Você é tão bom porque consegue decompor problemas difíceis em suas partes componentes, responder às partes componentes e depois juntá-las para responder à pergunta mais ampla.\\n\\n    Aqui está uma pergunta: {input}\\n    '), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001961EC5E6F0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001961E90A900>, root_client=<openai.OpenAI object at 0x000001961E453440>, root_async_client=<openai.AsyncOpenAI object at 0x000001961EC5E750>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}),\n",
       " 'História': LLMChain(verbose=True, prompt=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='\\n    Você é um historiador muito bom. Você tem um excelente conhecimento e compreensão de pessoas, eventos e contextos de uma variedade de períodos históricos. Você tem a capacidade de pensar, refletir, debater, discutir e avaliar o passado. Você tem respeito pela evidência histórica e a capacidade de usá-la para apoiar suas explicações e julgamentos.\\n\\n    \\n    Aqui está uma pergunta: {input}\\n    '), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001961EC5E6F0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001961E90A900>, root_client=<openai.OpenAI object at 0x000001961E453440>, root_async_client=<openai.AsyncOpenAI object at 0x000001961EC5E750>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={})}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chains_destino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "```json\n",
      "{{{{\n",
      "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}}}\n",
      "```\n",
      "\n",
      "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "{destinations}\n",
      "\n",
      "<< INPUT >>\n",
      "{{input}}\n",
      "\n",
      "<< OUTPUT (must include ```json at the start of the response) >>\n",
      "<< OUTPUT (must end with ```) >>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(MULTI_PROMPT_ROUTER_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisica: ideal para responder perguntas sobre física\n",
      "Matemática: ideal para responder perguntas sobre matemática\n",
      "História: ideal para responder perguntas sobre história\n"
     ]
    }
   ],
   "source": [
    "destinos = [\n",
    "    f'{p['name']}: {p['description']}' for p in prompt_infos\n",
    "]\n",
    "destinos_str = '\\n'.join(destinos)\n",
    "print(destinos_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinos_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "```json\n",
      "{{\n",
      "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}\n",
      "```\n",
      "\n",
      "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "Fisica: ideal para responder perguntas sobre física\n",
      "Matemática: ideal para responder perguntas sobre matemática\n",
      "História: ideal para responder perguntas sobre história\n",
      "\n",
      "<< INPUT >>\n",
      "{input}\n",
      "\n",
      "<< OUTPUT (must include ```json at the start of the response) >>\n",
      "<< OUTPUT (must end with ```) >>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(router_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = PromptTemplate(\n",
    "    template = router_template,\n",
    "    input_variables = ['input'],\n",
    "    output_parser=RouterOutputParser()\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(\n",
    "    chat,\n",
    "    router_template,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template('{input}')\n",
    "default_chain = LLMChain(llm=chat, prompt=default_prompt, verbose=True)\n",
    "\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=chains_destino,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMRouterChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "None: {'input': 'O que é um buraco negro?'}\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: O que é um buraco negro?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'O que é um buraco negro?',\n",
       " 'text': 'Um buraco negro é uma região do espaço onde a gravidade é tão intensa que nada, nem mesmo a luz, pode escapar de sua atração. Isso ocorre quando uma quantidade significativa de massa é compactada em um espaço muito pequeno, o que gera um campo gravitacional extremamente forte.\\n\\nOs buracos negros se formam geralmente a partir do colapso de estrelas massivas no final de suas vidas. Quando essas estrelas esgotam seu combustível nuclear, não conseguem mais suportar a força da gravidade e colapsam sob seu próprio peso. Esse colapso pode resultar em um buraco negro.\\n\\nExistem três tipos principais de buracos negros:\\n\\n1. **Buracos negros estelares**: Formados a partir do colapso de estrelas massivas. Eles têm algumas vezes algumas vezes a massa do Sol.\\n\\n2. **Buracos negros supermassivos**: Encontrados no centro de muitas galáxias, incluindo a Via Láctea. Eles podem ter milhões a bilhões de vezes a massa do Sol e a origem exata ainda é objeto de pesquisa.\\n\\n3. **Buracos negros intermediários**: Um tipo menos compreendido, que se acredita ter massas entre as dos buracos negros estelares e supermassivos.\\n\\nOs buracos negros são descritos pela Teoria da Relatividade Geral de Albert Einstein, que explica como a gravidade afeta o espaço-tempo. A \"fronteira\" ao redor de um buraco negro, além da qual nada pode escapar, é chamada de horizonte de eventos. Tudo que entra nesse horizonte não pode mais ser observado ou recuperado.\\n\\nEmbora não possamos ver os buracos negros diretamente, sua presença pode ser inferida através da observação de suas interações com a matéria ao seu redor, como a emissão de radiação de raios-X quando matéria é acelerada e aquecida antes de cair no buraco negro.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        'input': 'O que é um buraco negro?'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMRouterChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "None: {'input': 'O que é uma equação quadrática?'}\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: O que é uma equação quadrática?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'O que é uma equação quadrática?',\n",
       " 'text': 'Uma equação quadrática é uma equação polinomial de grau dois, que pode ser expressa na forma geral:\\n\\n\\\\[ ax^2 + bx + c = 0 \\\\]\\n\\nonde:\\n- \\\\( x \\\\) é a variável,\\n- \\\\( a \\\\), \\\\( b \\\\) e \\\\( c \\\\) são coeficientes reais (com \\\\( a \\\\neq 0 \\\\) para que a equação seja realmente quadrática).\\n\\nAs soluções da equação quadrática podem ser encontradas usando a formula de Bhaskara:\\n\\n\\\\[ x = \\\\frac{-b \\\\pm \\\\sqrt{b^2 - 4ac}}{2a} \\\\]\\n\\nO discriminante \\\\( \\\\Delta = b^2 - 4ac \\\\) determina o número e o tipo de soluções:\\n- Se \\\\( \\\\Delta > 0 \\\\), há duas soluções reais e distintas.\\n- Se \\\\( \\\\Delta = 0 \\\\), há uma solução real (ou duas soluções reais idênticas).\\n- Se \\\\( \\\\Delta < 0 \\\\), não há soluções reais, apenas soluções complexas.\\n\\nAs equações quadráticas são amplamente utilizadas em várias áreas da matemática e suas aplicações, incluindo física, economia e engenharia.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        'input': 'O que é uma equação quadrática?'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        'input': '?'\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
